{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"prefect-dask Welcome! Prefect integrations with the Dask.distributed library for distributed computing in Python. Provides a DaskTaskRunner that enables flows to run tasks requiring parallel or distributed execution using Dask. Getting Started Python setup Requires an installation of Python 3.7+. We recommend using a Python virtual environment manager such as pipenv, conda, or virtualenv. These tasks are designed to work with Prefect 2.0. For more information about how to use Prefect, please refer to the Prefect documentation . Installation Install prefect-dask with pip : pip install prefect-dask Running tasks on Dask The DaskTaskRunner is a parallel task runner that submits tasks to the dask.distributed scheduler. By default, a temporary Dask cluster is created for the duration of the flow run. For example, this flow says hello and goodbye in parallel. from prefect import flow , task from prefect_dask.task_runners import DaskTaskRunner from typing import List @task def say_hello ( name ): print ( f \"hello { name } \" ) @task def say_goodbye ( name ): print ( f \"goodbye { name } \" ) @flow ( task_runner = DaskTaskRunner ()) def greetings ( names : List [ str ]): for name in names : say_hello ( name ) say_goodbye ( name ) if __name__ == \"__main__\" : greetings ([ \"arthur\" , \"trillian\" , \"ford\" , \"marvin\" ]) # truncated output ... goodbye trillian goodbye arthur hello trillian hello ford hello marvin hello arthur goodbye ford goodbye marvin ... If you already have a Dask cluster running, either local or cloud hosted, you can provide the connection URL via an address argument. To configure your flow to use the DaskTaskRunner : Make sure the prefect-dask collection is installed as described earlier: pip install prefect-dask . In your flow code, import DaskTaskRunner from prefect_dask.task_runners . Assign it as the task runner when the flow is defined using the task_runner=DaskTaskRunner argument. For example, this flow uses the DaskTaskRunner configured to access an existing Dask cluster at http://my-dask-cluster . from prefect import flow from prefect_dask.task_runners import DaskTaskRunner @flow ( task_runner = DaskTaskRunner ( address = \"http://my-dask-cluster\" )) def my_flow (): ... DaskTaskRunner accepts the following optional parameters: Parameter Description address Address of a currently running Dask scheduler. cluster_class The cluster class to use when creating a temporary Dask cluster. It can be either the full class name (for example, \"distributed.LocalCluster\" ), or the class itself. cluster_kwargs Additional kwargs to pass to the cluster_class when creating a temporary Dask cluster. adapt_kwargs Additional kwargs to pass to cluster.adapt when creating a temporary Dask cluster. Note that adaptive scaling is only enabled if adapt_kwargs are provided. client_kwargs Additional kwargs to use when creating a dask.distributed.Client . Multiprocessing safety Note that, because the DaskTaskRunner uses multiprocessing, calls to flows in scripts must be guarded with if __name__ == \"__main__\": or you will encounter warnings and errors. If you don't provide the address of a Dask scheduler, Prefect creates a temporary local cluster automatically. The number of workers used is based on the number of cores available to your execution environment. The default provides a mix of processes and threads that should work well for most workloads. If you want to specify this explicitly, you can pass values for n_workers or threads_per_worker to cluster_kwargs . # Use 4 worker processes, each with 2 threads DaskTaskRunner ( cluster_kwargs = { \"n_workers\" : 4 , \"threads_per_worker\" : 2 } ) Using a temporary cluster The DaskTaskRunner is capable of creating a temporary cluster using any of Dask's cluster-manager options . This can be useful when you want each flow run to have its own Dask cluster, allowing for per-flow adaptive scaling. To configure, you need to provide a cluster_class . This can be: A string specifying the import path to the cluster class (for example, \"dask_cloudprovider.aws.FargateCluster\" ) The cluster class itself A function for creating a custom cluster You can also configure cluster_kwargs , which takes a dictionary of keyword arguments to pass to cluster_class when starting the flow run. For example, to configure a flow to use a temporary dask_cloudprovider.aws.FargateCluster with 4 workers running with an image named my-prefect-image : DaskTaskRunner ( cluster_class = \"dask_cloudprovider.aws.FargateCluster\" , cluster_kwargs = { \"n_workers\" : 4 , \"image\" : \"my-prefect-image\" }, ) Connecting to an existing cluster Multiple Prefect flow runs can all use the same existing Dask cluster. You might manage a single long-running Dask cluster (maybe using the Dask Helm Chart ) and configure flows to connect to it during execution. This has a few downsides when compared to using a temporary cluster (as described above): All workers in the cluster must have dependencies installed for all flows you intend to run. Multiple flow runs may compete for resources. Dask tries to do a good job sharing resources between tasks, but you may still run into issues. That said, you may prefer managing a single long-running cluster. To configure a DaskTaskRunner to connect to an existing cluster, pass in the address of the scheduler to the address argument: # Connect to an existing cluster running at a specified address DaskTaskRunner ( address = \"tcp://...\" ) Adaptive scaling One nice feature of using a DaskTaskRunner is the ability to scale adaptively to the workload. Instead of specifying n_workers as a fixed number, this lets you specify a minimum and maximum number of workers to use, and the dask cluster will scale up and down as needed. To do this, you can pass adapt_kwargs to DaskTaskRunner . This takes the following fields: maximum ( int or None , optional): the maximum number of workers to scale to. Set to None for no maximum. minimum ( int or None , optional): the minimum number of workers to scale to. Set to None for no minimum. For example, here we configure a flow to run on a FargateCluster scaling up to at most 10 workers. DaskTaskRunner ( cluster_class = \"dask_cloudprovider.aws.FargateCluster\" , adapt_kwargs = { \"maximum\" : 10 } ) Dask annotations Dask annotations can be used to further control the behavior of tasks. For example, we can set the priority of tasks in the Dask scheduler: import dask from prefect import flow , task from prefect_dask.task_runners import DaskTaskRunner @task def show ( x ): print ( x ) @flow ( task_runner = DaskTaskRunner ()) def my_flow (): with dask . annotate ( priority =- 10 ): future = show ( 1 ) # low priority task with dask . annotate ( priority = 10 ): future = show ( 2 ) # high priority task Another common use case is resource annotations: import dask from prefect import flow , task from prefect_dask.task_runners import DaskTaskRunner @task def show ( x ): print ( x ) # Create a `LocalCluster` with some resource annotations # Annotations are abstract in dask and not inferred from your system. # Here, we claim that our system has 1 GPU and 1 process available per worker @flow ( task_runner = DaskTaskRunner ( cluster_kwargs = { \"n_workers\" : 1 , \"resources\" : { \"GPU\" : 1 , \"process\" : 1 }} ) ) def my_flow (): with dask . annotate ( resources = { 'GPU' : 1 }): future = show ( 0 ) # this task requires 1 GPU resource on a worker with dask . annotate ( resources = { 'process' : 1 }): # These tasks each require 1 process on a worker; because we've # specified that our cluster has 1 process per worker and 1 worker, # these tasks will run sequentially future = show ( 1 ) future = show ( 2 ) future = show ( 3 ) Resources If you encounter any bugs while using prefect-dask , feel free to open an issue in the prefect-dask repository. If you have any questions or issues while using prefect-dask , you can find help in either the Prefect Discourse forum or the Prefect Slack community . Development If you'd like to install a version of prefect-dask for development, clone the repository and perform an editable install with pip : git clone https://github.com/PrefectHQ/prefect-dask.git cd prefect-dask/ pip install -e \".[dev]\" # Install linting pre-commit hooks pre-commit install","title":"Home"},{"location":"#prefect-dask","text":"","title":"prefect-dask"},{"location":"#welcome","text":"Prefect integrations with the Dask.distributed library for distributed computing in Python. Provides a DaskTaskRunner that enables flows to run tasks requiring parallel or distributed execution using Dask.","title":"Welcome!"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#python-setup","text":"Requires an installation of Python 3.7+. We recommend using a Python virtual environment manager such as pipenv, conda, or virtualenv. These tasks are designed to work with Prefect 2.0. For more information about how to use Prefect, please refer to the Prefect documentation .","title":"Python setup"},{"location":"#installation","text":"Install prefect-dask with pip : pip install prefect-dask","title":"Installation"},{"location":"#running-tasks-on-dask","text":"The DaskTaskRunner is a parallel task runner that submits tasks to the dask.distributed scheduler. By default, a temporary Dask cluster is created for the duration of the flow run. For example, this flow says hello and goodbye in parallel. from prefect import flow , task from prefect_dask.task_runners import DaskTaskRunner from typing import List @task def say_hello ( name ): print ( f \"hello { name } \" ) @task def say_goodbye ( name ): print ( f \"goodbye { name } \" ) @flow ( task_runner = DaskTaskRunner ()) def greetings ( names : List [ str ]): for name in names : say_hello ( name ) say_goodbye ( name ) if __name__ == \"__main__\" : greetings ([ \"arthur\" , \"trillian\" , \"ford\" , \"marvin\" ]) # truncated output ... goodbye trillian goodbye arthur hello trillian hello ford hello marvin hello arthur goodbye ford goodbye marvin ... If you already have a Dask cluster running, either local or cloud hosted, you can provide the connection URL via an address argument. To configure your flow to use the DaskTaskRunner : Make sure the prefect-dask collection is installed as described earlier: pip install prefect-dask . In your flow code, import DaskTaskRunner from prefect_dask.task_runners . Assign it as the task runner when the flow is defined using the task_runner=DaskTaskRunner argument. For example, this flow uses the DaskTaskRunner configured to access an existing Dask cluster at http://my-dask-cluster . from prefect import flow from prefect_dask.task_runners import DaskTaskRunner @flow ( task_runner = DaskTaskRunner ( address = \"http://my-dask-cluster\" )) def my_flow (): ... DaskTaskRunner accepts the following optional parameters: Parameter Description address Address of a currently running Dask scheduler. cluster_class The cluster class to use when creating a temporary Dask cluster. It can be either the full class name (for example, \"distributed.LocalCluster\" ), or the class itself. cluster_kwargs Additional kwargs to pass to the cluster_class when creating a temporary Dask cluster. adapt_kwargs Additional kwargs to pass to cluster.adapt when creating a temporary Dask cluster. Note that adaptive scaling is only enabled if adapt_kwargs are provided. client_kwargs Additional kwargs to use when creating a dask.distributed.Client . Multiprocessing safety Note that, because the DaskTaskRunner uses multiprocessing, calls to flows in scripts must be guarded with if __name__ == \"__main__\": or you will encounter warnings and errors. If you don't provide the address of a Dask scheduler, Prefect creates a temporary local cluster automatically. The number of workers used is based on the number of cores available to your execution environment. The default provides a mix of processes and threads that should work well for most workloads. If you want to specify this explicitly, you can pass values for n_workers or threads_per_worker to cluster_kwargs . # Use 4 worker processes, each with 2 threads DaskTaskRunner ( cluster_kwargs = { \"n_workers\" : 4 , \"threads_per_worker\" : 2 } )","title":"Running tasks on Dask"},{"location":"#using-a-temporary-cluster","text":"The DaskTaskRunner is capable of creating a temporary cluster using any of Dask's cluster-manager options . This can be useful when you want each flow run to have its own Dask cluster, allowing for per-flow adaptive scaling. To configure, you need to provide a cluster_class . This can be: A string specifying the import path to the cluster class (for example, \"dask_cloudprovider.aws.FargateCluster\" ) The cluster class itself A function for creating a custom cluster You can also configure cluster_kwargs , which takes a dictionary of keyword arguments to pass to cluster_class when starting the flow run. For example, to configure a flow to use a temporary dask_cloudprovider.aws.FargateCluster with 4 workers running with an image named my-prefect-image : DaskTaskRunner ( cluster_class = \"dask_cloudprovider.aws.FargateCluster\" , cluster_kwargs = { \"n_workers\" : 4 , \"image\" : \"my-prefect-image\" }, )","title":"Using a temporary cluster"},{"location":"#connecting-to-an-existing-cluster","text":"Multiple Prefect flow runs can all use the same existing Dask cluster. You might manage a single long-running Dask cluster (maybe using the Dask Helm Chart ) and configure flows to connect to it during execution. This has a few downsides when compared to using a temporary cluster (as described above): All workers in the cluster must have dependencies installed for all flows you intend to run. Multiple flow runs may compete for resources. Dask tries to do a good job sharing resources between tasks, but you may still run into issues. That said, you may prefer managing a single long-running cluster. To configure a DaskTaskRunner to connect to an existing cluster, pass in the address of the scheduler to the address argument: # Connect to an existing cluster running at a specified address DaskTaskRunner ( address = \"tcp://...\" )","title":"Connecting to an existing cluster"},{"location":"#adaptive-scaling","text":"One nice feature of using a DaskTaskRunner is the ability to scale adaptively to the workload. Instead of specifying n_workers as a fixed number, this lets you specify a minimum and maximum number of workers to use, and the dask cluster will scale up and down as needed. To do this, you can pass adapt_kwargs to DaskTaskRunner . This takes the following fields: maximum ( int or None , optional): the maximum number of workers to scale to. Set to None for no maximum. minimum ( int or None , optional): the minimum number of workers to scale to. Set to None for no minimum. For example, here we configure a flow to run on a FargateCluster scaling up to at most 10 workers. DaskTaskRunner ( cluster_class = \"dask_cloudprovider.aws.FargateCluster\" , adapt_kwargs = { \"maximum\" : 10 } )","title":"Adaptive scaling"},{"location":"#dask-annotations","text":"Dask annotations can be used to further control the behavior of tasks. For example, we can set the priority of tasks in the Dask scheduler: import dask from prefect import flow , task from prefect_dask.task_runners import DaskTaskRunner @task def show ( x ): print ( x ) @flow ( task_runner = DaskTaskRunner ()) def my_flow (): with dask . annotate ( priority =- 10 ): future = show ( 1 ) # low priority task with dask . annotate ( priority = 10 ): future = show ( 2 ) # high priority task Another common use case is resource annotations: import dask from prefect import flow , task from prefect_dask.task_runners import DaskTaskRunner @task def show ( x ): print ( x ) # Create a `LocalCluster` with some resource annotations # Annotations are abstract in dask and not inferred from your system. # Here, we claim that our system has 1 GPU and 1 process available per worker @flow ( task_runner = DaskTaskRunner ( cluster_kwargs = { \"n_workers\" : 1 , \"resources\" : { \"GPU\" : 1 , \"process\" : 1 }} ) ) def my_flow (): with dask . annotate ( resources = { 'GPU' : 1 }): future = show ( 0 ) # this task requires 1 GPU resource on a worker with dask . annotate ( resources = { 'process' : 1 }): # These tasks each require 1 process on a worker; because we've # specified that our cluster has 1 process per worker and 1 worker, # these tasks will run sequentially future = show ( 1 ) future = show ( 2 ) future = show ( 3 )","title":"Dask annotations"},{"location":"#resources","text":"If you encounter any bugs while using prefect-dask , feel free to open an issue in the prefect-dask repository. If you have any questions or issues while using prefect-dask , you can find help in either the Prefect Discourse forum or the Prefect Slack community .","title":"Resources"},{"location":"#development","text":"If you'd like to install a version of prefect-dask for development, clone the repository and perform an editable install with pip : git clone https://github.com/PrefectHQ/prefect-dask.git cd prefect-dask/ pip install -e \".[dev]\" # Install linting pre-commit hooks pre-commit install","title":"Development"},{"location":"task_runners/","text":"prefect_dask.task_runners Interface and implementations of the Dask Task Runner. Task Runners in Prefect are responsible for managing the execution of Prefect task runs. Generally speaking, users are not expected to interact with task runners outside of configuring and initializing them for a flow. Example from prefect import flow , task from prefect.task_runners import SequentialTaskRunner from typing import List @task def say_hello ( name ): print ( f \"hello { name } \" ) @task def say_goodbye ( name ): print ( f \"goodbye { name } \" ) @flow ( task_runner = SequentialTaskRunner ()) def greetings ( names : List [ str ]): for name in names : say_hello ( name ) say_goodbye ( name ) Switching to a DaskTaskRunner : from prefect_dask.task_runners import DaskTaskRunner flow . task_runner = DaskTaskRunner () greetings ([ \"arthur\" , \"trillian\" , \"ford\" , \"marvin\" ]) Output: hello arthur goodbye arthur hello trillian hello ford goodbye marvin hello marvin goodbye ford goodbye trillian DaskTaskRunner A parallel task_runner that submits tasks to the dask.distributed scheduler. By default a temporary distributed.LocalCluster is created (and subsequently torn down) within the start() contextmanager. To use a different cluster class (e.g. dask_kubernetes.KubeCluster ), you can specify cluster_class / cluster_kwargs . Alternatively, if you already have a dask cluster running, you can provide the address of the scheduler via the address kwarg. Multiprocessing safety Note that, because the DaskTaskRunner uses multiprocessing, calls to flows in scripts must be guarded with if __name__ == \"__main__\": or warnings will be displayed. Parameters: Name Type Description Default address string Address of a currently running dask scheduler; if one is not provided, a temporary cluster will be created in DaskTaskRunner.start() . Defaults to None . None cluster_class string or callable The cluster class to use when creating a temporary dask cluster. Can be either the full class name (e.g. \"distributed.LocalCluster\" ), or the class itself. None cluster_kwargs dict Additional kwargs to pass to the cluster_class when creating a temporary dask cluster. None adapt_kwargs dict Additional kwargs to pass to cluster.adapt when creating a temporary dask cluster. Note that adaptive scaling is only enabled if adapt_kwargs are provided. None client_kwargs dict Additional kwargs to use when creating a dask.distributed.Client . None Examples: Using a temporary local dask cluster: >>> from prefect import flow >>> from prefect_dask.task_runners import DaskTaskRunner >>> @flow ( task_runner = DaskTaskRunner ) >>> def my_flow (): >>> ... Using a temporary cluster running elsewhere. Any Dask cluster class should work, here we use [dask-cloudprovider](https://cloudprovider.dask.org): >>> DaskTaskRunner ( >>> cluster_class = \"dask_cloudprovider.FargateCluster\" , >>> cluster_kwargs = { >>> \"image\" : \"prefecthq/prefect:latest\" , >>> \"n_workers\" : 5 , >>> }, >>> ) Connecting to an existing dask cluster: >>> DaskTaskRunner ( address = \"192.0.2.255:8786\" ) Source code in prefect_dask/task_runners.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 class DaskTaskRunner ( BaseTaskRunner ): \"\"\" A parallel task_runner that submits tasks to the `dask.distributed` scheduler. By default a temporary `distributed.LocalCluster` is created (and subsequently torn down) within the `start()` contextmanager. To use a different cluster class (e.g. [`dask_kubernetes.KubeCluster`](https://kubernetes.dask.org/)), you can specify `cluster_class`/`cluster_kwargs`. Alternatively, if you already have a dask cluster running, you can provide the address of the scheduler via the `address` kwarg. !!! warning \"Multiprocessing safety\" Note that, because the `DaskTaskRunner` uses multiprocessing, calls to flows in scripts must be guarded with `if __name__ == \"__main__\":` or warnings will be displayed. Args: address (string, optional): Address of a currently running dask scheduler; if one is not provided, a temporary cluster will be created in `DaskTaskRunner.start()`. Defaults to `None`. cluster_class (string or callable, optional): The cluster class to use when creating a temporary dask cluster. Can be either the full class name (e.g. `\"distributed.LocalCluster\"`), or the class itself. cluster_kwargs (dict, optional): Additional kwargs to pass to the `cluster_class` when creating a temporary dask cluster. adapt_kwargs (dict, optional): Additional kwargs to pass to `cluster.adapt` when creating a temporary dask cluster. Note that adaptive scaling is only enabled if `adapt_kwargs` are provided. client_kwargs (dict, optional): Additional kwargs to use when creating a [`dask.distributed.Client`](https://distributed.dask.org/en/latest/api.html#client). Examples: Using a temporary local dask cluster: >>> from prefect import flow >>> from prefect_dask.task_runners import DaskTaskRunner >>> @flow(task_runner=DaskTaskRunner) >>> def my_flow(): >>> ... Using a temporary cluster running elsewhere. Any Dask cluster class should work, here we use [dask-cloudprovider](https://cloudprovider.dask.org): >>> DaskTaskRunner( >>> cluster_class=\"dask_cloudprovider.FargateCluster\", >>> cluster_kwargs={ >>> \"image\": \"prefecthq/prefect:latest\", >>> \"n_workers\": 5, >>> }, >>> ) Connecting to an existing dask cluster: >>> DaskTaskRunner(address=\"192.0.2.255:8786\") \"\"\" def __init__ ( self , address : str = None , cluster_class : Union [ str , Callable ] = None , cluster_kwargs : dict = None , adapt_kwargs : dict = None , client_kwargs : dict = None , ): # Validate settings and infer defaults if address : if cluster_class or cluster_kwargs or adapt_kwargs : raise ValueError ( \"Cannot specify `address` and \" \"`cluster_class`/`cluster_kwargs`/`adapt_kwargs`\" ) else : if isinstance ( cluster_class , str ): cluster_class = from_qualified_name ( cluster_class ) else : cluster_class = cluster_class # Create a copies of incoming kwargs since we may mutate them cluster_kwargs = cluster_kwargs . copy () if cluster_kwargs else {} adapt_kwargs = adapt_kwargs . copy () if adapt_kwargs else {} client_kwargs = client_kwargs . copy () if client_kwargs else {} # Update kwargs defaults client_kwargs . setdefault ( \"set_as_default\" , False ) # The user cannot specify async/sync themselves if \"asynchronous\" in client_kwargs : raise ValueError ( \"`client_kwargs` cannot set `asynchronous`. \" \"This option is managed by Prefect.\" ) if \"asynchronous\" in cluster_kwargs : raise ValueError ( \"`cluster_kwargs` cannot set `asynchronous`. \" \"This option is managed by Prefect.\" ) # Store settings self . address = address self . cluster_class = cluster_class self . cluster_kwargs = cluster_kwargs self . adapt_kwargs = adapt_kwargs self . client_kwargs = client_kwargs # Runtime attributes self . _client : \"distributed.Client\" = None self . _cluster : \"distributed.deploy.Cluster\" = None self . _dask_futures : Dict [ str , \"distributed.Future\" ] = {} super () . __init__ () @property def concurrency_type ( self ) -> TaskConcurrencyType : return ( TaskConcurrencyType . PARALLEL if self . cluster_kwargs . get ( \"processes\" ) else TaskConcurrencyType . CONCURRENT ) async def submit ( self , task_run : TaskRun , run_key : str , run_fn : Callable [ ... , Awaitable [ State [ R ]]], run_kwargs : Dict [ str , Any ], asynchronous : A = True , ) -> PrefectFuture [ R , A ]: if not self . _started : raise RuntimeError ( \"The task runner must be started before submitting work.\" ) # Cast Prefect futures to Dask futures where possible to optimize Dask task # scheduling run_kwargs = await self . _optimize_futures ( run_kwargs ) self . _dask_futures [ run_key ] = self . _client . submit ( run_fn , # Dask displays the text up to the first '-' as the name, the task run key # should include the task run name for readability in the dask console. key = run_key , # Dask defaults to treating functions are pure, but we set this here for # explicit expectations. If this task run is submitted to Dask twice, the # result of the first run should be returned. Subsequent runs would return # `Abort` exceptions if they were submitted again. pure = True , ** run_kwargs , ) return PrefectFuture ( task_run = task_run , task_runner = self , run_key = run_key , asynchronous = asynchronous , ) def _get_dask_future ( self , prefect_future : PrefectFuture ) -> \"distributed.Future\" : \"\"\" Retrieve the dask future corresponding to a Prefect future. The Dask future is for the `run_fn`, which should return a `State`. \"\"\" return self . _dask_futures [ prefect_future . run_key ] async def _optimize_futures ( self , expr ): async def visit_fn ( expr ): if isinstance ( expr , PrefectFuture ): dask_future = self . _dask_futures . get ( expr . run_key ) if dask_future is not None : return dask_future # Fallback to return the expression unaltered return expr return await visit_collection ( expr , visit_fn = visit_fn , return_data = True ) async def wait ( self , prefect_future : PrefectFuture , timeout : float = None , ) -> Optional [ State ]: future = self . _get_dask_future ( prefect_future ) try : return await future . result ( timeout = timeout ) except distributed . TimeoutError : return None except BaseException as exc : return exception_to_crashed_state ( exc ) async def _start ( self , exit_stack : AsyncExitStack ): \"\"\" Start the task runner and prep for context exit. - Creates a cluster if an external address is not set. - Creates a client to connect to the cluster. - Pushes a call to wait for all running futures to complete on exit. \"\"\" if self . address : self . logger . info ( f \"Connecting to an existing Dask cluster at { self . address } \" ) connect_to = self . address else : self . cluster_class = self . cluster_class or distributed . LocalCluster self . logger . info ( f \"Creating a new Dask cluster with \" f \"` { to_qualified_name ( self . cluster_class ) } `\" ) connect_to = self . _cluster = await exit_stack . enter_async_context ( self . cluster_class ( asynchronous = True , ** self . cluster_kwargs ) ) if self . adapt_kwargs : self . _cluster . adapt ( ** self . adapt_kwargs ) self . _client = await exit_stack . enter_async_context ( distributed . Client ( connect_to , asynchronous = True , ** self . client_kwargs ) ) if self . _client . dashboard_link : self . logger . info ( f \"The Dask dashboard is available at { self . _client . dashboard_link } \" , ) def __getstate__ ( self ): \"\"\" Allow the `DaskTaskRunner` to be serialized by dropping the `distributed.Client`, which contains locks. Must be deserialized on a dask worker. \"\"\" data = self . __dict__ . copy () data . update ({ k : None for k in { \"_client\" , \"_cluster\" }}) return data def __setstate__ ( self , data : dict ): \"\"\" Restore the `distributed.Client` by loading the client on a dask worker. \"\"\" self . __dict__ . update ( data ) self . _client = distributed . get_client () __getstate__ Allow the DaskTaskRunner to be serialized by dropping the distributed.Client , which contains locks. Must be deserialized on a dask worker. Source code in prefect_dask/task_runners.py 276 277 278 279 280 281 282 283 284 def __getstate__ ( self ): \"\"\" Allow the `DaskTaskRunner` to be serialized by dropping the `distributed.Client`, which contains locks. Must be deserialized on a dask worker. \"\"\" data = self . __dict__ . copy () data . update ({ k : None for k in { \"_client\" , \"_cluster\" }}) return data __setstate__ Restore the distributed.Client by loading the client on a dask worker. Source code in prefect_dask/task_runners.py 286 287 288 289 290 291 def __setstate__ ( self , data : dict ): \"\"\" Restore the `distributed.Client` by loading the client on a dask worker. \"\"\" self . __dict__ . update ( data ) self . _client = distributed . get_client ()","title":"Task Runners"},{"location":"task_runners/#prefect_dask.task_runners","text":"Interface and implementations of the Dask Task Runner. Task Runners in Prefect are responsible for managing the execution of Prefect task runs. Generally speaking, users are not expected to interact with task runners outside of configuring and initializing them for a flow. Example from prefect import flow , task from prefect.task_runners import SequentialTaskRunner from typing import List @task def say_hello ( name ): print ( f \"hello { name } \" ) @task def say_goodbye ( name ): print ( f \"goodbye { name } \" ) @flow ( task_runner = SequentialTaskRunner ()) def greetings ( names : List [ str ]): for name in names : say_hello ( name ) say_goodbye ( name ) Switching to a DaskTaskRunner : from prefect_dask.task_runners import DaskTaskRunner flow . task_runner = DaskTaskRunner () greetings ([ \"arthur\" , \"trillian\" , \"ford\" , \"marvin\" ]) Output: hello arthur goodbye arthur hello trillian hello ford goodbye marvin hello marvin goodbye ford goodbye trillian","title":"task_runners"},{"location":"task_runners/#prefect_dask.task_runners.DaskTaskRunner","text":"A parallel task_runner that submits tasks to the dask.distributed scheduler. By default a temporary distributed.LocalCluster is created (and subsequently torn down) within the start() contextmanager. To use a different cluster class (e.g. dask_kubernetes.KubeCluster ), you can specify cluster_class / cluster_kwargs . Alternatively, if you already have a dask cluster running, you can provide the address of the scheduler via the address kwarg. Multiprocessing safety Note that, because the DaskTaskRunner uses multiprocessing, calls to flows in scripts must be guarded with if __name__ == \"__main__\": or warnings will be displayed. Parameters: Name Type Description Default address string Address of a currently running dask scheduler; if one is not provided, a temporary cluster will be created in DaskTaskRunner.start() . Defaults to None . None cluster_class string or callable The cluster class to use when creating a temporary dask cluster. Can be either the full class name (e.g. \"distributed.LocalCluster\" ), or the class itself. None cluster_kwargs dict Additional kwargs to pass to the cluster_class when creating a temporary dask cluster. None adapt_kwargs dict Additional kwargs to pass to cluster.adapt when creating a temporary dask cluster. Note that adaptive scaling is only enabled if adapt_kwargs are provided. None client_kwargs dict Additional kwargs to use when creating a dask.distributed.Client . None Examples: Using a temporary local dask cluster: >>> from prefect import flow >>> from prefect_dask.task_runners import DaskTaskRunner >>> @flow ( task_runner = DaskTaskRunner ) >>> def my_flow (): >>> ... Using a temporary cluster running elsewhere. Any Dask cluster class should work, here we use [dask-cloudprovider](https://cloudprovider.dask.org): >>> DaskTaskRunner ( >>> cluster_class = \"dask_cloudprovider.FargateCluster\" , >>> cluster_kwargs = { >>> \"image\" : \"prefecthq/prefect:latest\" , >>> \"n_workers\" : 5 , >>> }, >>> ) Connecting to an existing dask cluster: >>> DaskTaskRunner ( address = \"192.0.2.255:8786\" ) Source code in prefect_dask/task_runners.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 class DaskTaskRunner ( BaseTaskRunner ): \"\"\" A parallel task_runner that submits tasks to the `dask.distributed` scheduler. By default a temporary `distributed.LocalCluster` is created (and subsequently torn down) within the `start()` contextmanager. To use a different cluster class (e.g. [`dask_kubernetes.KubeCluster`](https://kubernetes.dask.org/)), you can specify `cluster_class`/`cluster_kwargs`. Alternatively, if you already have a dask cluster running, you can provide the address of the scheduler via the `address` kwarg. !!! warning \"Multiprocessing safety\" Note that, because the `DaskTaskRunner` uses multiprocessing, calls to flows in scripts must be guarded with `if __name__ == \"__main__\":` or warnings will be displayed. Args: address (string, optional): Address of a currently running dask scheduler; if one is not provided, a temporary cluster will be created in `DaskTaskRunner.start()`. Defaults to `None`. cluster_class (string or callable, optional): The cluster class to use when creating a temporary dask cluster. Can be either the full class name (e.g. `\"distributed.LocalCluster\"`), or the class itself. cluster_kwargs (dict, optional): Additional kwargs to pass to the `cluster_class` when creating a temporary dask cluster. adapt_kwargs (dict, optional): Additional kwargs to pass to `cluster.adapt` when creating a temporary dask cluster. Note that adaptive scaling is only enabled if `adapt_kwargs` are provided. client_kwargs (dict, optional): Additional kwargs to use when creating a [`dask.distributed.Client`](https://distributed.dask.org/en/latest/api.html#client). Examples: Using a temporary local dask cluster: >>> from prefect import flow >>> from prefect_dask.task_runners import DaskTaskRunner >>> @flow(task_runner=DaskTaskRunner) >>> def my_flow(): >>> ... Using a temporary cluster running elsewhere. Any Dask cluster class should work, here we use [dask-cloudprovider](https://cloudprovider.dask.org): >>> DaskTaskRunner( >>> cluster_class=\"dask_cloudprovider.FargateCluster\", >>> cluster_kwargs={ >>> \"image\": \"prefecthq/prefect:latest\", >>> \"n_workers\": 5, >>> }, >>> ) Connecting to an existing dask cluster: >>> DaskTaskRunner(address=\"192.0.2.255:8786\") \"\"\" def __init__ ( self , address : str = None , cluster_class : Union [ str , Callable ] = None , cluster_kwargs : dict = None , adapt_kwargs : dict = None , client_kwargs : dict = None , ): # Validate settings and infer defaults if address : if cluster_class or cluster_kwargs or adapt_kwargs : raise ValueError ( \"Cannot specify `address` and \" \"`cluster_class`/`cluster_kwargs`/`adapt_kwargs`\" ) else : if isinstance ( cluster_class , str ): cluster_class = from_qualified_name ( cluster_class ) else : cluster_class = cluster_class # Create a copies of incoming kwargs since we may mutate them cluster_kwargs = cluster_kwargs . copy () if cluster_kwargs else {} adapt_kwargs = adapt_kwargs . copy () if adapt_kwargs else {} client_kwargs = client_kwargs . copy () if client_kwargs else {} # Update kwargs defaults client_kwargs . setdefault ( \"set_as_default\" , False ) # The user cannot specify async/sync themselves if \"asynchronous\" in client_kwargs : raise ValueError ( \"`client_kwargs` cannot set `asynchronous`. \" \"This option is managed by Prefect.\" ) if \"asynchronous\" in cluster_kwargs : raise ValueError ( \"`cluster_kwargs` cannot set `asynchronous`. \" \"This option is managed by Prefect.\" ) # Store settings self . address = address self . cluster_class = cluster_class self . cluster_kwargs = cluster_kwargs self . adapt_kwargs = adapt_kwargs self . client_kwargs = client_kwargs # Runtime attributes self . _client : \"distributed.Client\" = None self . _cluster : \"distributed.deploy.Cluster\" = None self . _dask_futures : Dict [ str , \"distributed.Future\" ] = {} super () . __init__ () @property def concurrency_type ( self ) -> TaskConcurrencyType : return ( TaskConcurrencyType . PARALLEL if self . cluster_kwargs . get ( \"processes\" ) else TaskConcurrencyType . CONCURRENT ) async def submit ( self , task_run : TaskRun , run_key : str , run_fn : Callable [ ... , Awaitable [ State [ R ]]], run_kwargs : Dict [ str , Any ], asynchronous : A = True , ) -> PrefectFuture [ R , A ]: if not self . _started : raise RuntimeError ( \"The task runner must be started before submitting work.\" ) # Cast Prefect futures to Dask futures where possible to optimize Dask task # scheduling run_kwargs = await self . _optimize_futures ( run_kwargs ) self . _dask_futures [ run_key ] = self . _client . submit ( run_fn , # Dask displays the text up to the first '-' as the name, the task run key # should include the task run name for readability in the dask console. key = run_key , # Dask defaults to treating functions are pure, but we set this here for # explicit expectations. If this task run is submitted to Dask twice, the # result of the first run should be returned. Subsequent runs would return # `Abort` exceptions if they were submitted again. pure = True , ** run_kwargs , ) return PrefectFuture ( task_run = task_run , task_runner = self , run_key = run_key , asynchronous = asynchronous , ) def _get_dask_future ( self , prefect_future : PrefectFuture ) -> \"distributed.Future\" : \"\"\" Retrieve the dask future corresponding to a Prefect future. The Dask future is for the `run_fn`, which should return a `State`. \"\"\" return self . _dask_futures [ prefect_future . run_key ] async def _optimize_futures ( self , expr ): async def visit_fn ( expr ): if isinstance ( expr , PrefectFuture ): dask_future = self . _dask_futures . get ( expr . run_key ) if dask_future is not None : return dask_future # Fallback to return the expression unaltered return expr return await visit_collection ( expr , visit_fn = visit_fn , return_data = True ) async def wait ( self , prefect_future : PrefectFuture , timeout : float = None , ) -> Optional [ State ]: future = self . _get_dask_future ( prefect_future ) try : return await future . result ( timeout = timeout ) except distributed . TimeoutError : return None except BaseException as exc : return exception_to_crashed_state ( exc ) async def _start ( self , exit_stack : AsyncExitStack ): \"\"\" Start the task runner and prep for context exit. - Creates a cluster if an external address is not set. - Creates a client to connect to the cluster. - Pushes a call to wait for all running futures to complete on exit. \"\"\" if self . address : self . logger . info ( f \"Connecting to an existing Dask cluster at { self . address } \" ) connect_to = self . address else : self . cluster_class = self . cluster_class or distributed . LocalCluster self . logger . info ( f \"Creating a new Dask cluster with \" f \"` { to_qualified_name ( self . cluster_class ) } `\" ) connect_to = self . _cluster = await exit_stack . enter_async_context ( self . cluster_class ( asynchronous = True , ** self . cluster_kwargs ) ) if self . adapt_kwargs : self . _cluster . adapt ( ** self . adapt_kwargs ) self . _client = await exit_stack . enter_async_context ( distributed . Client ( connect_to , asynchronous = True , ** self . client_kwargs ) ) if self . _client . dashboard_link : self . logger . info ( f \"The Dask dashboard is available at { self . _client . dashboard_link } \" , ) def __getstate__ ( self ): \"\"\" Allow the `DaskTaskRunner` to be serialized by dropping the `distributed.Client`, which contains locks. Must be deserialized on a dask worker. \"\"\" data = self . __dict__ . copy () data . update ({ k : None for k in { \"_client\" , \"_cluster\" }}) return data def __setstate__ ( self , data : dict ): \"\"\" Restore the `distributed.Client` by loading the client on a dask worker. \"\"\" self . __dict__ . update ( data ) self . _client = distributed . get_client ()","title":"DaskTaskRunner"},{"location":"task_runners/#prefect_dask.task_runners.DaskTaskRunner.__getstate__","text":"Allow the DaskTaskRunner to be serialized by dropping the distributed.Client , which contains locks. Must be deserialized on a dask worker. Source code in prefect_dask/task_runners.py 276 277 278 279 280 281 282 283 284 def __getstate__ ( self ): \"\"\" Allow the `DaskTaskRunner` to be serialized by dropping the `distributed.Client`, which contains locks. Must be deserialized on a dask worker. \"\"\" data = self . __dict__ . copy () data . update ({ k : None for k in { \"_client\" , \"_cluster\" }}) return data","title":"__getstate__()"},{"location":"task_runners/#prefect_dask.task_runners.DaskTaskRunner.__setstate__","text":"Restore the distributed.Client by loading the client on a dask worker. Source code in prefect_dask/task_runners.py 286 287 288 289 290 291 def __setstate__ ( self , data : dict ): \"\"\" Restore the `distributed.Client` by loading the client on a dask worker. \"\"\" self . __dict__ . update ( data ) self . _client = distributed . get_client ()","title":"__setstate__()"}]}